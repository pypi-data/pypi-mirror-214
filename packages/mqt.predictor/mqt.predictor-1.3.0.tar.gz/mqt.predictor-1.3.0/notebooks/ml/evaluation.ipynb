{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5189fbc1",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eeb4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from numpy import asarray, save\n",
    "from mqt.predictor import utils, ml\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "predictor = ml.Predictor()\n",
    "\n",
    "(X_train,\n",
    "X_test,\n",
    "y_train,\n",
    "y_test,\n",
    "indices_train,\n",
    "indices_test,\n",
    "names_list,\n",
    "scores_list) = predictor.get_prepared_training_data(save_non_zero_indices=True)\n",
    "\n",
    "scores_filtered = [scores_list[i] for i in indices_test]\n",
    "names_filtered = [names_list[i] for i in indices_test]\n",
    "\n",
    "performance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0932e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453a219",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "tree_param = [\n",
    "    {\n",
    "        \"n_estimators\": [100, 200, 500],\n",
    "        \"max_depth\": list(range(8, 30, 6)),\n",
    "        \"min_samples_split\": list(range(2, 20, 6)),\n",
    "        \"min_samples_leaf\": list(range(2, 20, 6)),\n",
    "        \"bootstrap\": [True, False],\n",
    "    },\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(clf, tree_param, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.plot_eval_histogram(res, filename=\"RandomForestClassifier\")\n",
    "rel_goodness = np.round(np.mean(rel_scores),4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores),4)\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Feature Importance: \", clf.best_estimator_.feature_importances_)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "performance.append((\"Random Forest\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.plot_eval_all_detailed_compact_normed(\n",
    "    names_filtered, scores_filtered, y_pred, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab679fa",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = ml.helper.get_path_trained_model() / \"non_zero_indices.npy\"\n",
    "non_zero_indices = np.load(str(path), allow_pickle=True)\n",
    "        \n",
    "openqasm_qc_list = ml.helper.get_openqasm_gates()\n",
    "feature_names = [openqasm_qc_list[i] for i in range(0, len(openqasm_qc_list))]\n",
    "feature_names.append(\"num_qubits\")\n",
    "feature_names.append(\"depth\")\n",
    "feature_names.append(\"program_communication\")\n",
    "feature_names.append(\"critical_depth\")\n",
    "feature_names.append(\"entanglement_ratio\")\n",
    "feature_names.append(\"parallelism\")\n",
    "feature_names.append(\"liveness\")\n",
    "feature_names = [feature_names[i] for i in non_zero_indices]\n",
    "\n",
    "importances = clf.best_estimator_.feature_importances_\n",
    "std = np.std(\n",
    "    [tree.feature_importances_ for tree in clf.best_estimator_.estimators_], axis=0\n",
    ")\n",
    "\n",
    "idx = np.argsort(-importances)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.bar(np.array(feature_names)[idx], np.array(importances)[idx])\n",
    "plt.errorbar(\n",
    "    np.array(feature_names)[idx],\n",
    "    np.array(importances)[idx],\n",
    "    np.array(std)[idx],\n",
    "    fmt=\"o\",\n",
    "    color=\"#ff8600\",\n",
    ")\n",
    "plt.xticks(rotation=90, fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.ylabel(\"Relative feature importance\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/feature_importances.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd88baf",
   "metadata": {},
   "source": [
    "#### Check the relative importances per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b77209",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = zip(np.array(feature_names)[idx], np.array(importances)[idx])\n",
    "for feature, importance in list(summary):\n",
    "    print(feature, np.round(importance,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253d80e",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aca3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.plot_eval_histogram(res, filename=\"GradientBoostingClassifier\")\n",
    "rel_goodness = np.round(np.mean(rel_scores),4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores),4)\n",
    "\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "performance.append((\"Gradient Boosting\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9aacd",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f88a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(random_state=5)\n",
    "\n",
    "tree_param = [\n",
    "    {\n",
    "        \"criterion\": [\"entropy\", \"gini\"],\n",
    "        \"max_depth\": list(range(1, 15, 1)),\n",
    "        \"min_samples_split\": list(range(2, 20, 4)),\n",
    "        \"min_samples_leaf\": list(range(2, 20, 4)),\n",
    "        \"max_leaf_nodes\": list(range(2, 200, 40)),\n",
    "        \"max_features\": list(range(1, len(non_zero_indices), 10)),\n",
    "    },\n",
    "]\n",
    "clf = GridSearchCV(clf, tree_param, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.plot_eval_histogram(res, filename=\"DecisionTreeClassifier\")\n",
    "rel_goodness = np.round(np.mean(rel_scores),4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores),4)\n",
    "\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "print(\"Feature Importance: \", clf.best_estimator_.feature_importances_)\n",
    "performance.append((\"Decision Tree\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b7f2a",
   "metadata": {},
   "source": [
    "# Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a9f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "param_grid = dict(n_neighbors=range(1, 10, 1))\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.plot_eval_histogram(res, filename=\"KNeighborsClassifier\")\n",
    "rel_goodness = np.round(np.mean(rel_scores),4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores),4)\n",
    "\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "performance.append((\"Nearest Neighbor\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a215cbd",
   "metadata": {},
   "source": [
    "# MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(max_iter=1000)\n",
    "\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [(50, 50, 50), (50, 100, 50), (100,)],\n",
    "    \"activation\": [\"tanh\", \"relu\"],\n",
    "    \"solver\": [\"sgd\", \"adam\"],\n",
    "    \"alpha\": [0.0001, 0.05],\n",
    "    \"learning_rate\": [\"constant\", \"adaptive\"],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.plot_eval_histogram(res, filename=\"MLPClassifier\")\n",
    "rel_goodness = np.round(np.mean(rel_scores),4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores),4)\n",
    "\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "performance.append((\"Multilayer Perceptron\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e7f1a",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb259b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "param_grid = {\"C\": [0.1, 1, 10], \"gamma\": [1, 0.1, 0.01], \"kernel\": [\"rbf\", \"sigmoid\"]}\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.plot_eval_histogram(res, filename=\"SVM\")\n",
    "rel_goodness = np.round(np.mean(rel_scores),4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores),4)\n",
    "\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "performance.append((\"Support Vector Machine\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbac0a1",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04146e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "param_grid = {\"var_smoothing\": np.logspace(0, -9, num=100)}\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.plot_eval_histogram(res, filename=\"GaussianNB\")\n",
    "rel_goodness = np.round(np.mean(rel_scores),4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores),4)\n",
    "\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "performance.append((\"Naive Bayes\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ea3c3",
   "metadata": {},
   "source": [
    "# Save Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e522a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(performance)\n",
    "\n",
    "filename = \"results/performances.csv\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"Classifier, Accuracy, Top3, Worst Rank, Eval. Score Diff., Std\\n\")\n",
    "    for sublist in performance:\n",
    "        line = \"{}, {}, {}, {}, {}, {} \\n\".format(\n",
    "            sublist[0], sublist[1], sublist[2], sublist[3], sublist[4], sublist[5]\n",
    "        )\n",
    "        f.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
